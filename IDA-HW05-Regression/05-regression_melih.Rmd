---
title: "Homework Sheet 5 -- Regression modeling"
date: 'Due: Wednesday, January 20 by 11:59 CET'
author: ""
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# package for Bayesian regression
library(brms)

# parallel execution of Stan code
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')
```

# Instructions

**Important!** The `aida` package has been updated. Please run the following code to make sure your local version has these updates.

```{r, eval = F}
remotes::install_github("michael-franke/aida-package")
```

* Create an Rmd-file with your group number (equivalent to StudIP group) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA_HW05-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
   * an R Markdown file "IDA_HW05-Group-XYZ.Rmd"
   * a knitted HTML document "IDA_HW05-Group-XYZ.html"
   * any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in your group folder before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.
* If you run `brms::brm` in an R code chunk, make sure to include `results='hide'` as an option to that code chunk to suppress the output of Stan.

# <span style = "color:firebrick">Exercise 1:</span> Difference coding in Mental Chronometry [20 points]

The purpose of this exercise is to better understand coding schemes for categorical factors.
We also would like to get more comfortable with testing hypotheses about regression coefficients in a Bayesian setting.
Towards this end, we will construct a design matrix by hand to implement **simple difference coding**.
We use the non-informative Bayesian regression model to efficiently sample from the posterior and test hypotheses about the Mental Chronometry data.
In particular we are interested in the hypotheses spelled out in Appendix D.1.1.2 of the web-book, namely:

- mean reaction times are lower for data from the 'reaction' block than for the 'goNoGo' block
- mean reaction times are lower for data from the 'goNoGo' block than for the 'discrimination' block

## Ex 1.a Prepare data [1 point]

Create a variable `data_MC_excerpt` that contains all and only the relevant columns from the MC data set.
The dependent variable should be the first column.

```{r, eval = F}
# dependent: RT
data_MC_excerpt <- aida::data_MC_cleaned %>% select(RT, trial_number, block)
```

**Solution**
The dependent variable is the reaction time and the relevant columns from the data are, the reaction time, submission_id, trial number and the independent variable block.


## Ex 1.b Get the empirical differences between means [2 points]


Compute the following differences:

1. the mean RT for condition `goNoGo` minus the mean RT for condition `reaction`
2. the mean RT for condition `discrimination` minus the mean RT for condition `goNoGo`

and store the result in the variables provided below (keep the brackets so that the values assigned show in the Rmarkdown output):

```{r, eval = F}
means <- data_MC_excerpt %>%
  group_by(block) %>%
  summarise( means = mean(RT)) %>%
  pivot_wider(names_from = block, values_from = means)

(`goNoGo-reaction`       <-  means$goNoGo - means$reaction)
(`discrimination-goNoGo` <-  means$discrimination - means$goNoGo)

```

**Solution:**

## Ex 1.c Build predictor matrix [5 points]

Add three columns to the data tibble `data_MC_excerpt`, like shown below.
These columns should contain the numerical values to obtain so-called **simple difference coding** (as discussed in the initial video of Chapter 14, after ca. 10 minutes).

```{r, eval = F}
data_MC_excerpt <- data_MC_excerpt %>% 
  mutate(
    grand_mean  = 1,
    `goNoGo-reaction` = case_when(
      block == "reaction" ~ -2/3,
      block == "discrimination" ~ 1/3,
      block == "goNoGo" ~ 1/3
    ), 
    `discrimination-goNoGo` = case_when(
      block == "reaction" ~ -1/3,
      block == "discrimination" ~ 2/3,
      block == "goNoGo" ~ 1/3
    )
)
```


**Solution:**


## Ex 1.d Run a Bayesian regression model [3 points]

Use the function `aida::get_samples_regression_noninformative` to obtain 10,000 samples from the posterior distribution for a standard linear regression model.
(Hint: Use `as.matrix` to extract the predictor matrix from the tibble you created in the previous step.)

**Solution:**
```{r}
X <- data_MC_excerpt %>% 
  select(grand_mean, `goNoGo-reaction`, `discrimination-goNoGo`) %>% 
  as.matrix()

posterior_distribution_samples <- aida::get_samples_regression_noninformative(X, data_MC_excerpt$RT, 10000)
```



## Ex 1.e Summary statistics [2 point]


Use the function `aida::summarize_sample_vector` to show Bayesian summary statistics for the two most relevant parameters (given the hypotheses we are interested in for the Mental Chronometry experiment).

**Solution:**
```{r}
discGoDiff <- aida::summarize_sample_vector(samples_Bayes_regression$`discrimination-goNoGo`, name = "discrimination-goNoGo")
goRecDiff <- aida::summarize_sample_vector(samples_Bayes_regression$`goNoGo-reaction`, name = "goNoGo-reaction")

full_join(discGoDiff, goRecDiff)
```


## Ex 1.f Interpret your results [7 points]

Interpret these results using a Kruschke-style logic applied to (non-ROPEd) interval-based hypotheses.
In doing so, state clearly what the coefficients encode, what the numerical results of the last step mean, and what you would conclude from all this regarding any potential evidence (or lack thereof) for or against the research hypotheses.

**Solution:**
- mean reaction times are lower for data from the ‘reaction’ block than for the ‘goNoGo’ block
- mean reaction times are lower for data from the ‘goNoGo’ block than for the ‘discrimination’ block



# <span style = "color:firebrick">Exercise 2:</span> Analyzing the King of France [28 points]

The purpose of this exercise is to practice with logistic regression and the interpretation of interaction terms.
Thanks to those who took part, we have data from the King of France experiment from students of this class, so we can engage in "self-analysis".
If you want to understand what this experiment is about, read the relevant appendix chapter in the web-book.

We will be interested in the following research questions:

- **H1**: The latent probability of "TRUE" judgements is higher in C0 (with presupposition) than in C1 (where the presupposition is part of the at-issue / asserted content).
- **H2**: There is no difference in truth-value judgements between C0 (the positive sentence) and C6 (the negative sentence).
- **H3**: The disposition towards "TRUE" judgements is lower for C9 (where the presupposition is topical) than for C10 (where the presupposition is not topical and occurs under negation).

Load the data as follows (also applying some massaging, similar to what's done in the web-book):

```{r }
IDA_data_KoF <- read_csv('data_KoF-IDA-2020.csv') %>% 
  # discard practice trials
  filter(type != "practice") %>% 
  mutate(
    # add a 'condition' variable
    condition = case_when(
      type == "special" ~ "background check",
      type == "main" ~ str_c("Condition ", condition),
      TRUE ~ "filler"
    ) %>% 
      factor( 
        ordered = T,
        levels = c(str_c("Condition ", c(0, 1, 6, 9, 10)), "background check", "filler")
      )
  ) %>% 
  rename(correct_answer = correct)
```

## Ex 2.a Clean the data [3 points]

Just like in the web-book we are going to clean the data by performing the following two steps in sequence:

1. Remove all data from any participant who got more than 50% of the answer to filler material wrong.
2. Remove individual main trials if the corresponding "background check" question was answered wrongly.

Use code from the web-book at will, but be mindful that you may have to make (minor) changes (e.g., variable naming).

Use R code to report how many participants (in Step 1) and trials (in step 2) are excluded.

**Solution:**
```{r}
inital_part <- length(unique(IDA_data_KoF$submission_id))

IDA_data_KoF <- IDA_data_KoF %>% group_by(submission_id) %>% 
  mutate(
    filleraAnswerRatio = mean(condition == "filler" & correct_answer == "TRUE")/mean(condition == "filler")
  ) %>% 
  filter(filleraAnswerRatio >= 0.5) %>% 
  select(-filleraAnswerRatio)

firstFilter <- IDA_data_KoF %>% group_by(submission_id) %>% 
  summarize(nParts = mean(RT)) %>% nrow()
firstFilterTrials <- IDA_data_KoF %>% group_by(submission_id) %>% nrow()

IDA_data_KoF <- IDA_data_KoF %>% 
  filter(condition != "background check" | (condition == "background check" & correct_answer == "TRUE"))

secondFilter <- IDA_data_KoF %>% group_by(submission_id) %>% nrow()
firstFilterTrials - secondFilter



# look at error rates for filler sentences by subject
# mark every subject with < 0.5 proportion correct

subject_error_rate <- data_KoF_processed %>% 
  filter(trial_type == "filler") %>% 
  group_by(submission_id) %>% 
  summarise(
    proportion_correct = mean(correct_answer == response),
    outlier_subject = proportion_correct < 0.5
  ) %>% 
  arrange(proportion_correct)



IDA_data_KoF %>% 
    # 'reorder' answer categories (making 'correct' the target to be explained)
    mutate(response = response == 'TRUE')

```


## Ex 2.b Plot the data [2 points]

Plot the data just like in the the first part of the web-book's appendix D4.4 (proportion per condition). 
Use the web-book code as much as you like while making any necessary amendments.

**Solution:**
```{r}
IDA_data_KoF%>% 
  # drop unused factor levels
  droplevels() %>% 
  # get means and 95% bootstrapped CIs for each condition
  group_by(condition) %>%
  nest() %>% 
  summarise(
    CIs = map(data, function(d) bootstrapped_CI(d$response == "TRUE"))
  ) %>% 
  unnest(CIs) %>% 
  # plot means and CIs
  ggplot(aes(x = condition, y = mean, fill = condition)) + 
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper, width = 0.2)) +
  ylim(0,1) +
  ylab("") + xlab("") + ggtitle("Proportion of 'TRUE' responses per condition") +
  theme(legend.position = "none") +
  scale_fill_manual(values = c(1,2,3,4,5,6,7))



IDA_data_KoF
```


## Ex 2.c Run logistic regression model [5 points]

Run a logistic regression model on the cleaned data, predicting `response` in terms of categorical factor `condition`.
Make sure to also do the following:

- make the ordered factor `condition` a character vector before running the regression model
- change the non-informative default priors for all slope coefficients to a Student's t distribution with 1 degree of freedom, a mean of 0 and a standard deviation of 2 (see web-book Chapter 13.4)
- set the flap `sample_prior = 'yes'` to also sample from the prior
- take 20,000 samples (using parameter `iter` in the function call of `brm`)

Finally, also add a call of `summary(...)` to show the summary of the fitted model in the Rmarkdown output.

**Solution:**
```{r}
fit_brms_ST_Acc = brm(
  # regress 'correctness' against 'condition'
  formula = response ~ as.character(condition),
  # specify link and likeihood function
  family = bernoulli(link = "logit"),
  # which data to use
  data = IDA_data_KoF %>% 
    # 'reorder' answer categories (making 'correct' the target to be explained)
    mutate(response = response == 'TRUE'),
  # weakly informative priors (slightly conservative)
  #   for `class = 'b'` (i.e., all slopes)
  prior = prior(student_t(1, 0, 2)),
  # also collect samples from the prior (for point-valued testing)
  sample_prior = 'yes',
  # take more than the usual samples (for numerical stability of testing)
  iter = 20000
)

summary(fit_brms_ST_Acc)$fixed[,c("l-95% CI", "Estimate", "u-95% CI")]

```


## Ex 2.d Test hypotheses [12 points]

Use the `brms::hypotheses` function to test the three hypotheses stated above.
For each case interpret the results and formulate a conclusion such as "does/does not provide evidence" or "accept/reject hypothesis" according to the results.

### Hypothesis 1: C0 > C1
**H1**: The latent probability of "TRUE" judgements is higher in C0 (with presupposition) than in C1 (where the presupposition is part of the at-issue / asserted content).

**Solution:**
```{r}
brms::hypothesis(fit_brms_ST_Acc, "as.characterconditionCondition0 > as.characterconditionCondition1")
```
From the slope variables we can observe that the mean values of condition 0 and condition 1 do not vary by much . Additionally the evidence ratio of the hypothesis is relatively small. Given the small evidence ratio we cannot be certain that the hypothesis is completely true or not true.


### Hypothesis 2: C0 = C6
- **H2**: There is no difference in truth-value judgements between C0 (the positive sentence) and C6 (the negative sentence).

**Solution:**
```{r}
brms::hypothesis(fit_brms_ST_Acc, "as.characterconditionCondition0 = as.characterconditionCondition6")
```
Again the slope variables containing each condition do not match. And with a evidence ratio of zero there is no proof for the hypothesis. 


### Hypothesis 3: C9 < C10
- **H3**: The disposition towards "TRUE" judgements is lower for C9 (where the presupposition is topical) than for C10 (where the presupposition is not topical and occurs under negation).

**Solution:**
```{r}
brms::hypothesis(fit_brms_ST_Acc, "as.characterconditionCondition9 < as.characterconditionCondition10")
```
In this case the respective slope variables and the evidence ratio give stong evidence that the hypothesis is in fact true. 



## Ex 2.e Interaction with `gender` [6 points]

Let's get nasty!
Let's do what no good scientist should ever do. (We do it for practice and for rubbing it in that this is NOT what you should EVER do.)

Suppose we did not like the outcome of the test for H1 above.
So we are going to run a new analysis (for which there is no *prima facie* rationale) just to see if we cannot possibly squeeze out something that looks like an argument for the conclusion that we so crave to be supported.
Worst of all, we stipulate -completely out of the blue- a potential gender effect on the understanding of linguistic presupposition (say what?).

Obviously, what this exercise is really after is getting practice with interpreting *interaction terms* for categorical regression.
So, look at the code below, try to understand what it does and then inspect the summary of the model fit shown here.
Then answer the following question: based on this analysis and results shown in the summary, is there any reason to believe that (i) there is a difference in the answer behavior between self-identified male and self-identified female participants (i.e., is there a main effect of `gender`?), and (ii) is there any specific way in which gender impacts on the disposition to select "TRUE" in either of the two conditions (i.e., is there an interaction?).
Identify the source of your conclusion by pointing out which numbers (or whatever) you draw on in the summary of the model fit.

```{r, results = 'hide'}
IDA_data_KoF_interaction <- IDA_data_KoF %>% 
  filter(condition %in% c("Condition 0", "Condition 1")) %>% 
  filter(!is.na(gender)) %>% 
  mutate(
    gender = factor(gender)
  ) %>% 
  droplevels() %>% 
  select(response, condition, gender)
  
# apply 'sum' contrasts
contrasts(IDA_data_KoF_interaction$condition) <- contr.sum(2)
contrasts(IDA_data_KoF_interaction$gender) <- contr.sum(2)

# add intelligible name to the new contrast coding
colnames(contrasts(IDA_data_KoF_interaction$condition)) <- ":Cond1"
colnames(contrasts(IDA_data_KoF_interaction$gender)) <- ":male"

fit_brms_KoF_interaction <- brm(
  # predict `response` in terms of `condition`
  formula = response ~ condition * gender,
  # specify which data to use
  data = IDA_data_KoF_interaction,
  # logistic regression
  family = bernoulli(link = "logit"),
  # weakly informative priors (slightly conservative)
  #   for `class = 'b'` (i.e., all slopes)
  prior = prior(student_t(1, 0, 2),  class = 'b'),
  # also collect samples from the prior (for point-valued testing)
  sample_prior = 'yes',
  # take more than the usual samples (for numerical stability of testing)
  iter = 20000
)
```


```{r}
summary(fit_brms_KoF_interaction)
```

**Solution:**
**H1** There is a difference in the answer behavior between self-identified male and self-identified female participants (i.e., is there a main effect of `gender`?)
```{r}
brms::hypothesis(fit_brms_KoF_interaction, "(2*Intercept+condition:Cond1) < (2*Intercept+ 2*gender:male+condition:Cond1:gender:male+condition:Cond1)")
```
Given the value of the evidence ratio there is slight evidence suggesting a difference in precision between gender when answering the questions of condition 0 and condition 1.


**H2** Is there any specific way in which gender impacts on the disposition to select "TRUE" in either of the two conditions (i.e., is there an interaction?).
```{r}
brms::hypothesis(fit_brms_KoF_interaction, "-condition:Cond1:gender:male < 2*gender:male+condition:Cond1:gender:male") 
```


```{r}
IDA_data_KoF_interaction %>% 
  group_by(gender) %>% 
  summarize(
    True_condition0 = mean(response == "TRUE" & condition == "Condition 0")/mean(condition == "Condition 0"),
    True_condition1 = mean(response == "TRUE" & condition == "Condition 1")/mean(condition == "Condition 1")
    )
```
Men are too ignorant!!!! They always press TRUE without knowing if it realy is. This is why I filed my third divorce!!!  



